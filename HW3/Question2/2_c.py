# -*- coding: utf-8 -*-
"""2_c.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HeSFtQt0X4yxLvuIlPVXfcEPt4mCdDgX
"""

from scipy.stats import norm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import minimize
import math
import matplotlib.pyplot as plt
from scipy.special import gamma
from scipy.stats import norm, t
import numpy as np
from scipy.stats import norm

data_train = pd.read_csv("bank-note/train.csv", header=None)
data_test = pd.read_csv("bank-note/test.csv", header=None)
X = data_train.iloc[:,:4]
Y = data_train.iloc[:,-1]
X_test = data_test.iloc[:,:4]
Y_test = data_test.iloc[:,-1]

def probit_regression(X, t):
    w = np.zeros(X.shape[1])
    alpha = 1
    max_iter = 100
    epsilon = 1e-5
    lambd = 1
    I = np.identity(X.shape[1])
    for i in range(max_iter):
        z = np.dot(X, w)
        # y= 1.0 / (1.0 + np.exp(-z))
        y = norm.cdf(z)
        grad = np.dot(X.T, y - t) + w
        H = np.dot(X.T * norm.pdf(z), X) + I
        w_new = w - alpha * np.dot(np.linalg.inv(H), grad)
        if np.linalg.norm(w_new - w) < epsilon:
            break
        w = w_new
    return w
def eval(data, labels, weights):
    c = 0
    for i in range(data.shape[0]):
        z = np.dot(np.array(data.iloc[i]), weights)
        # y= norm.cdf(z)
        y = np.random.normal(z,1)
        if y>=0:
          if 1 == labels.iloc[i]:
              c+=1
        else:
          if 0 == labels.iloc[i]:
              c+=1
    return c

np.random.seed(2)
weights = probit_regression(X,Y)
print("With Initializing the weights to be zero: ")
print("The converged weights are : ",weights)
print("Train Accuracy ",eval(X, Y, weights)/len(Y))
print("Test Accuracy ",eval(X_test, Y_test, weights)/len(Y_test))

